{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial11: PLLaVAè§†é¢‘ç†è§£æ¨¡å‹æ¨ç†\n",
    "\n",
    "å»ºè®®åœ¨SCOW AIé›†ç¾¤è¿è¡Œæœ¬æ•™ç¨‹ã€‚æ¨èä½¿ç”¨[PLLaVA-NPUåº“](https://github.com/SunnyMass/PLLaVA-NPU)çš„ä»£ç ã€‚\n",
    "\n",
    "æœ¬èŠ‚æ—¨åœ¨ä½¿ç”¨ [pllava-7b](https://huggingface.co/ermu2001/pllava-7b) æ¨¡å‹å±•ç¤ºå¤šæ¨¡æ€è§†é¢‘ç†è§£æ¨¡å‹çš„æ¨ç†ã€‚\n",
    "\n",
    "åˆ†ä»¥ä¸‹å‡ æ­¥æ¥å®ç°ï¼š\n",
    "1. ç¯å¢ƒå®‰è£…\n",
    "2. ä¸‹è½½æ¨¡å‹\n",
    "3. 3Dæ± åŒ–ç®—å­é€‚é…\n",
    "4. æ¨¡å‹æ¨ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå®‰è£…\n",
    "\n",
    "å»ºè®®ä½¿ç”¨1å¼ 910B NPUè¿è¡Œæœ¬æ•™ç¨‹ã€‚\n",
    "\n",
    "æŒ‰ç…§å¦‚ä¸‹æ–¹å¼åˆ›å»ºcondaè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶å®‰è£…æ‰€éœ€åº“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "conda create -n pllava python=3.10\n",
    "conda activate pllava\n",
    "pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ä¸‹è½½æ¨¡å‹\n",
    "\n",
    "æŒ‰ç…§å¦‚ä¸‹æ–¹å¼ä¸‹è½½pllava-7bæ¨¡å‹ã€‚\n",
    "\n",
    "```bash\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "huggingface-cli download --resume-download ermu2001/pllava-7b --local-dir pllava-7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3Dæ± åŒ–ç®—å­é€‚é…\n",
    "\n",
    "åŸå§‹ä»£ç é—®é¢˜åˆ†æï¼š\n",
    "\n",
    "RuntimeError: adaptive_avg_pool3d only support D=1 && H=1 && W=1 current!è¯¥é—®é¢˜æŒ‡å‡ºï¼ŒNPUä¸æ”¯æŒ3Dæ± åŒ–ï¼Œå³ä¸æ”¯æŒtorch._C._nn.adaptive_avg_pool3dè‡ªé€‚åº”å°ºå¯¸ã€‚å¦‚æœè§£å†³è¿™ç§æŠ¥é”™ï¼Œå¿…é¡»è¦ä¿®æ”¹å°ºå¯¸ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹æ¨ç†æ—¶ï¼Œå‡ºç°é”™è¯¯çš„æ˜ å°„å¹¶æ— æ³•å¯¹é½ï¼Œä»è€Œå¯¼è‡´å¹»è§‰ç­‰é—®é¢˜å‡ºç°ã€‚å¦‚æœæ”¹æˆ2Dæ± åŒ–æˆ–è€…ç”¨å…¶ä»–æ–¹å¼è¡¨å¾æ—¶åŸŸï¼Œéƒ½æ— æ³•æ”¹å–„è¿™ç§å¹»è§‰é—®é¢˜ï¼Œè€Œåœ¨GPUä¸Šæ¨ç†èƒ½å¾—åˆ°å‡†ç¡®ç»“æœã€‚å› æ­¤éœ€è¦ä»æœ¬è´¨ä¸Šè§£å†³è¿™ä¸ª3Dæ± åŒ–å‡½æ•°é€‚é…é—®é¢˜ã€‚\n",
    "AdaptiveAvgPool3d((T_out, H_out, W_out)) è¡¨ç¤ºï¼šåœ¨ä¸‰ç»´ç©ºé—´ï¼ˆæ—¶é—´ + ç©ºé—´ï¼‰ä¸Šï¼Œè‡ªåŠ¨å°†æ¯æ®µ videoï¼ˆå½¢çŠ¶ [C, T, H, W]ï¼‰å¹³å‡æ± åŒ–åˆ°æŒ‡å®šå½¢çŠ¶ [C, T_out, H_out, W_out]ã€‚å¦‚æœè¾“å…¥æ˜¯ [B, C, T=8, H=14, W=14]ï¼Œè®¾ç½® --pooling_shape 4-12-12ï¼Œå°±ä¼šå˜æˆï¼šè¾“å‡ºå½¢çŠ¶ = [B, C, 4, 12, 12]ï¼Œä»è€Œç¨€ç–åŒ–tokenä¸ªæ•°ã€‚\n",
    "\n",
    "\n",
    "è§£å†³æ–¹æ¡ˆï¼š\n",
    "\n",
    "ç”±äº NPU ä¸æ”¯æŒåŒ…æ‹¬ AdaptiveAvgPool3d åœ¨å†…çš„å¤šç§ 3D æ± åŒ–ç®—å­ï¼Œå› æ­¤æˆ‘ä»¬æ‰‹åŠ¨å®ç°ä¸€ä¸ªå…¼å®¹ NPU çš„ 3D æ± åŒ–å‡½æ•°ï¼Œç”¨äºåœ¨ä¸æ”¹å˜æ•°å€¼è¯­ä¹‰çš„å‰æä¸‹ï¼Œå®Œæˆæ—¶é—´ç»´åº¦å’Œç©ºé—´ç»´åº¦çš„é™é‡‡æ ·ï¼Œç¡®ä¿æ¨ç†è¿‡ç¨‹ä¸ GPU ä¸Šä¸€è‡´ï¼Œé¿å…å‡ºç°æ˜ å°„é”™ä¹±æˆ–è¯­ä¹‰å¹»è§‰ç­‰é—®é¢˜ã€‚è¯¥å‡½æ•°å®Œå…¨å¤ç°äº† AdaptiveAvgPool3d çš„è¡Œä¸ºï¼Œå‰ææ˜¯è¾“å…¥çš„æ—¶é—´ã€ç©ºé—´å°ºå¯¸èƒ½æ•´é™¤ç›®æ ‡å°ºå¯¸ã€‚é€‚ç”¨äºå¤§å¤šæ•°å®é™…é…ç½®ï¼ŒåŒæ—¶åœ¨ NPU ä¸Šé«˜æ•ˆå¯ç”¨ï¼Œç¡®ä¿è§†è§‰ç‰¹å¾ token æ•°åœ¨æ—¶ç©ºç»´åº¦çš„ç¨€ç–åŒ–è¿‡ç¨‹ä¿æŒä¸€è‡´ã€‚\n",
    "\n",
    "åœ¨**PLLaVA/models/pllava/modeling_pllava.py**æ–‡ä»¶ä¸­ä¿®æ”¹ï¼ŒæŠŠclass PllavaMultiModalProjector(nn.Module):ä¸­çš„self.poolingæ›¿æ¢ä¸ºä¸‹é¢çš„è‡ªå®šä¹‰å®ç°ã€‚è¯¥å®ç°å·²ç»éªŒè¯åœ¨GPUå’ŒNPUçš„è¾“å‡ºä¿æŒä¸€è‡´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def adaptive_avg_pool3d_manual(self, x, output_size):\n",
    "        \"\"\"\n",
    "        x: [B, C, D, H, W]\n",
    "        output_size: (d_out, h_out, w_out)\n",
    "        æ›¿ä»£ AdaptiveAvgPool3dï¼ŒNPU å…¼å®¹\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = x.shape\n",
    "        d_out, h_out, w_out = output_size\n",
    "        print(output_size)\n",
    "        assert D % d_out == 0 and H % h_out == 0 and W % w_out == 0, \"Input size must be divisible by output size\"\n",
    "\n",
    "        kd = D // d_out\n",
    "        kh = H // h_out\n",
    "        kw = W // w_out\n",
    "\n",
    "        # reshape æˆ 6ç»´ï¼šå°† D/H/W åˆ†æˆ avg block å— + å—å†…å…ƒç´ \n",
    "        x = x.view(B, C, d_out, kd, h_out, kh, w_out, kw)  # [B, C, d_out, kd, h_out, kh, w_out, kw]\n",
    "        x = x.mean(dim=(3, 5, 7))  # å¯¹ kd, kh, kw ä¸‰ä¸ªç»´åº¦åšå‡å€¼\n",
    "        return x  # shape [B, C, d_out, h_out, w_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹æ¨ç†\n",
    "\n",
    "PLLaVA-7B æ˜¯ä¸€ä¸ªé¢å‘è§†é¢‘ç†è§£ä»»åŠ¡çš„å¼€æºå¤šæ¨¡æ€èŠå¤©æ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨å›¾åƒå¤§æ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥å¼•å…¥è§†é¢‘æŒ‡ä»¤æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå…·å¤‡è§†é¢‘å†…å®¹ç†è§£ä¸é—®ç­”èƒ½åŠ›ã€‚å…¶åº•å±‚è¯­è¨€æ¨¡å‹ä¸º llava-hf/llava-v1.6-vicuna-7b-hfï¼Œé‡‡ç”¨ Transformer æ¶æ„ï¼Œå…·å¤‡è‡ªå›å½’ç”Ÿæˆèƒ½åŠ›ã€‚\n",
    "\n",
    "æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ npu-smi info å‘½ä»¤å¯ä»¥æŸ¥çœ‹ NPU è¿è¡Œæƒ…å†µã€‚\n",
    "\n",
    "æ¨èç›´æ¥ä½¿ç”¨[PLLaVA-NPUåº“](https://github.com/SunnyMass/PLLaVA-NPU)çš„ä»£ç è¿è¡Œã€‚\n",
    "\n",
    "è¿è¡Œå‘½ä»¤ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "python run_demo.py   --video_path path_to_1-2.mp4   --prompt \"describe this video in detail\"   --pretrained_model_name_or_path path_to_pllava7b   --weight_dir path_to_pllava7b   --use_lora   --num_frames 16   --conv_mode plain   --max_new_tokens 128  --video_caption(å¦‚æœæ˜¯åšè§†é¢‘captionä»»åŠ¡å°±åŠ ä¸Šï¼Œå¦‚æœæ˜¯å…¶ä»–è§†é¢‘ç†è§£ä»»åŠ¡å°±ä¸åŠ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tasks.eval.eval_utils import conv_templates, ChatPllava\n",
    "from tasks.eval.model_utils import load_pllava\n",
    "\n",
    "SYSTEM = \"\"\"You are a powerful Video Magic ChatBot, a large vision-language assistant. \n",
    "You are able to understand the video content that the user provides and assist the user in a video-language related task.\n",
    "The user might provide you with the video and maybe some extra noisy information to help you out or ask you a question. Make use of the information in a proper way to be competent for the job.\n",
    "### INSTRUCTIONS:\n",
    "1. Follow the user's instruction.\n",
    "2. Be critical yet believe in yourself.\n",
    "\"\"\"\n",
    "SYSTEM2 = \"\"\"\n",
    "Describe this video. Pay attention to all objects in the video. The description should be useful for AI to re-generate the video. The description should be no more than six sentences. Here are some examples of good descriptions: 1. A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about. 2. Several giant wooly mammoths approach treading through a snowy meadow, their long wooly fur lightly blows in the wind as they walk, snow covered trees and dramatic snow capped mountains in the distance, mid afternoon light with wispy clouds and a sun high in the distance creates a warm glow, the low camera view is stunning capturing the large furry mammal with beautiful photography, depth of field. 3. Drone view of waves crashing against the rugged cliffs along Big Sur's garay point beach. The crashing blue waters create white-tipped waves, while the golden light of the setting sun illuminates the rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery covers the cliff's edge. The steep drop from the road down to the beach is a dramatic feat, with the cliffâ€™s edges jutting out over the sea. This is a view that captures the raw beauty of the coast and the rugged landscape of the Pacific Coast Highway.\n",
    "\"\"\"\n",
    "\n",
    "def load_video(video_path, num_segments=4, resolution=336):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    indices = [int(i * total_frames / num_segments) for i in range(num_segments)]\n",
    "\n",
    "    frames = []\n",
    "    resize = transforms.Resize((resolution, resolution))\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx in indices:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame)\n",
    "            pil_img = resize(pil_img)\n",
    "            frames.append(pil_img)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--video_path', type=str, required=True)\n",
    "    parser.add_argument('--prompt', type=str, required=True)\n",
    "    parser.add_argument('--num_frames', type=int, default=4)\n",
    "    parser.add_argument('--pretrained_model_name_or_path', type=str, required=True)\n",
    "    parser.add_argument('--weight_dir', type=str, default=None)\n",
    "    parser.add_argument('--use_lora', action='store_true')\n",
    "    parser.add_argument('--lora_alpha', type=int, default=4)\n",
    "    parser.add_argument('--conv_mode', type=str, default='plain')\n",
    "    parser.add_argument('--max_new_tokens', type=int, default=200)\n",
    "    parser.add_argument('--num_beams', type=int, default=1)\n",
    "    parser.add_argument('--temperature', type=float, default=1.0)\n",
    "    parser.add_argument('--video_caption', action='store_true')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    print(\"ğŸ“¦ Loading model...\")\n",
    "    model, processor = load_pllava(\n",
    "        repo_id=args.pretrained_model_name_or_path,\n",
    "        num_frames=args.num_frames,\n",
    "        use_lora=args.use_lora,\n",
    "        lora_alpha=args.lora_alpha,\n",
    "        weight_dir=args.weight_dir,\n",
    "    )\n",
    "    model = model.to('npu').eval()\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    chat = ChatPllava(model, processor)\n",
    "\n",
    "    print(\"ğŸ“½ï¸ Loading video frames...\")\n",
    "    frames = load_video(args.video_path, args.num_frames)\n",
    "    img_list = [frames]  # å¿…é¡»æ˜¯äºŒç»´åˆ—è¡¨ [ [PIL, PIL, PIL...] ]\n",
    "\n",
    "    print(\"ğŸ’¬ Asking and answering...\")\n",
    "    conv = conv_templates[args.conv_mode].copy()\n",
    "    if args.video_caption:\n",
    "        conv = chat.ask(args.prompt, conv, SYSTEM2)\n",
    "    else:\n",
    "        conv = chat.ask(args.prompt, conv, SYSTEM)\n",
    "\n",
    "    start_time = time.time()\n",
    "    llm_message, _, _ = chat.answer(\n",
    "        conv=conv,\n",
    "        img_list=img_list,\n",
    "        max_new_tokens=args.max_new_tokens,\n",
    "        num_beams=args.num_beams,\n",
    "        temperature=args.temperature\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nâ±ï¸ Inference took {elapsed:.2f} seconds\")\n",
    "    print(\"\\n===== FINAL ANSWER =====\\n\")\n",
    "    print(llm_message.strip())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœéœ€è¦éƒ¨ç½²gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sh ./scripts/demo.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯ä»¥åœ¨PLLaVA/tasks/eval/demo/pllava_demo.pyæ–‡ä»¶æœ«å°¾å®šä¹‰urlåœ°å€ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "demo.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=10034,\n",
    "    root_path=\"/ai/api/proxy/ascend-k8s/relative/master/30003/proxy/10034\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gradio](https://github.com/PKUHPC/scow-for-ai-tutorial-ascend/raw/tutorial11/tutorial11_PLLaVAè§†é¢‘ç†è§£æ¨¡å‹æ¨ç†/gradio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> ä½œè€…: å¼ å¤©æˆˆ; æ—è£ç¾¤; è´¾å·æ°‘; é©¬æ€ä¼Ÿ\n",
    ">\n",
    "> è”ç³»æ–¹å¼: tgzhang@stu.pku.edu.cn"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
