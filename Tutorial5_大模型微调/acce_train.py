from transformers import AutoTokenizer, AutoModelForCausalLM
from accelerate import Accelerator
from torch.utils.data import DataLoader
from data_prepare import CPMDataset
import torch
import time

# 数据准备
trainset = CPMDataset("bee_data/train.jsonl")  
train_loader = DataLoader(trainset, batch_size=1)

# 模型
model_path = "cpm-bee-2b"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)

# 训练
optimizer = torch.optim.Adam(model.parameters())

accelerator = Accelerator()
model, optimizer, train_loader = accelerator.prepare(
    model, optimizer, train_loader
)

# 计时
total_time = 0

for iter, data in enumerate(train_loader):
    
    model.train()
    
    step_start = time.perf_counter()
    
    # 训练
    optimizer.zero_grad()
    input_encoded = tokenizer.prepare_for_finetune(data, max_length=512)
    outputs = model(**input_encoded)
    loss = outputs.loss
    accelerator.backward(loss)
    optimizer.step()
    
    step_time = time.perf_counter() - step_start
    total_time += step_time
    
    # 仅在主进程输出
    if accelerator.is_main_process:
        print(f"Step {iter}, Loss: {loss.item():.4f}, Time per step: {step_time:.4f} s")

    
# 仅在主进程输出
if accelerator.is_main_process:
    print("Training done")
    print(f"Total training time: {total_time:.2f} s")
    print(f"Average time per step: {total_time / (iter + 1):.4f} s")